{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emo_Bert_Training\n",
    "\n",
    "This Notebook has been run on Google Colab with a Premium Account\n",
    "\n",
    "**Input :** Solution.pkl\n",
    "\n",
    "**main  :** Training and testing our pre-trained Bert Classifier for emotions on our novel dataset.\n",
    "        Running Vader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "colab_type": "code",
    "id": "HMyWdJVmhmLw",
    "outputId": "f97567cf-23ee-4cef-c05c-c5acbe39a5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 4.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.4)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.13.4)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 14.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.5.0+cu101)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 24.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.16.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->pytorch-transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->pytorch-transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=5d9df22d9689653892fa882be37713613372848dcf63563119206ba346ebe8f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.90\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHgYBfbDzR-2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XE_6nvgmhxoB"
   },
   "outputs": [],
   "source": [
    "# After eager execution is enabled, operations are executed as they are\n",
    "# defined and Tensor objects hold concrete values, which can be accessed as\n",
    "# numpy.ndarray`s through the numpy() method.\n",
    "assert tf.multiply(6, 7).numpy() == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "bKX0oPIBiEKC",
    "outputId": "c52deba5-8d23-4494-a0e7-cf23e5ab546a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "drive.mount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ybLBkm0mtHUT",
    "outputId": "93fe27c0-bbec-409e-9675-078cbad57d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8vix6fNiKcA"
   },
   "outputs": [],
   "source": [
    "labels = ['afraid',\n",
    "            'angry',\n",
    "            'annoyed',\n",
    "            'anticipating',\n",
    "            'anxious',\n",
    "            'apprehensive',\n",
    "            'ashamed',\n",
    "            'caring',\n",
    "            'confident',\n",
    "            'content',\n",
    "            'devastated',\n",
    "            'disappointed',\n",
    "            'disgusted',\n",
    "            'embarrassed',\n",
    "            'excited',\n",
    "            'faithful',\n",
    "            'furious',\n",
    "            'grateful',\n",
    "            'guilty',\n",
    "            'hopeful',\n",
    "            'impressed',\n",
    "            'jealous',\n",
    "            'joyful',\n",
    "            'lonely',\n",
    "            'nostalgic',\n",
    "            'prepared',\n",
    "            'proud',\n",
    "            'sad',\n",
    "            'sentimental',\n",
    "            'surprised',\n",
    "            'terrified',\n",
    "            'trusting',\n",
    "            'agreeing', # neutral in emotion\n",
    "            'acknowledging', # neutral in emotion\n",
    "            'encouraging', # neutral in emotion\n",
    "            'consoling', # neutral in emotion\n",
    "            'sympathizing', # neutral in emotion\n",
    "            'suggesting', # neutral in emotion\n",
    "            'questioning', # neutral in emotion\n",
    "            'wishing', # neutral in emotion\n",
    "            'neutral'] # neutral in emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYoHYbd6ijg0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate, name = 'multi_head_attention'):\n",
    "        super().__init__(name = name)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, name = 'query')\n",
    "        self.wk = tf.keras.layers.Dense(d_model, name = 'key')\n",
    "        self.wv = tf.keras.layers.Dense(d_model, name = 'value')\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate, name = 'mha_dropout')\n",
    "        self.dense = tf.keras.layers.Dense(d_model, name = 'mha_output')\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        \"\"\"\n",
    "        Calculate the attention weights.\n",
    "        q, k, v must have matching leading dimensions.\n",
    "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "        The mask has different shapes depending on its type(padding or look ahead) \n",
    "        but it must be broadcastable for addition.\n",
    "        Args:\n",
    "            q: query shape == (..., seq_len_q, depth)\n",
    "            k: key shape == (..., seq_len_k, depth)\n",
    "            v: value shape == (..., seq_len_v, depth_v)\n",
    "            mask: Float tensor with shape broadcastable \n",
    "                to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            output, attention_weights\n",
    "        \"\"\"\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        # (As claimed in the RoBERTa implementation.)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + tf.math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * tf.math.pow(x, 3))))\n",
    "        Also see https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + tf.math.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "act_funcs = {'gelu': gelu, 'relu': tf.nn.relu}\n",
    "\n",
    "# Pointwise Feed Forward Network\n",
    "def point_wise_feed_forward_network(d_model, dff, hidden_act):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation = act_funcs[hidden_act],\n",
    "            name = 'ff_hidden'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model, name = 'ff_output')  # (batch_size, seq_len, d_model)\n",
    "    ], name = 'ff_network')\n",
    "\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, hidden_act, dropout_rate, layer_norm_eps, layer_num):\n",
    "        super().__init__(name = 'encoder_layer_{:02d}'.format(layer_num))\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff, hidden_act)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = layer_norm_eps,\n",
    "            name = 'layernorm_1')\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = layer_norm_eps,\n",
    "            name = 'layernorm_2')\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate, name = 'dropout_1')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate, name = 'dropout_2')\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-IBYR3HinCB"
   },
   "outputs": [],
   "source": [
    "def loss_function(real_emot, pred_emot):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True, reduction = 'none')\n",
    "    loss_ = scce(real_emot, pred_emot)\n",
    "    return loss_\n",
    "\n",
    "class EmoBERT(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, hidden_act, dropout_rate,\n",
    "                 layer_norm_eps, max_position_embed, vocab_size, num_emotions):\n",
    "        super().__init__(name = 'emo_bert')\n",
    "\n",
    "        self.padding_idx = 1\n",
    "\n",
    "        # Embedding layers\n",
    "        self.word_embeddings = tf.keras.layers.Embedding(vocab_size, d_model, name = 'word_embed')\n",
    "        self.pos_embeddings = tf.keras.layers.Embedding(max_position_embed, d_model, name = 'pos_embed')\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon = layer_norm_eps,\n",
    "            name = 'layernorm_embed')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate, name = 'dropout_embed')\n",
    "\n",
    "        # Encoder layers\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, hidden_act, dropout_rate, layer_norm_eps, i)\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output layers\n",
    "        self.attention_v = tf.keras.layers.Dense(1, use_bias = False, name = 'attention_v')\n",
    "        self.attention_layer = tf.keras.layers.Dense(d_model, activation = 'tanh', name = 'attention_layer')\n",
    "        self.hidden_layer = tf.keras.layers.Dense(d_model, activation = 'tanh', name = 'hidden_layer')\n",
    "        self.output_layer = tf.keras.layers.Dense(num_emotions, name = 'output_layer')\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # x.shape == (batch_size, seq_len)\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # Add word embedding and position embedding.\n",
    "        pos = tf.range(self.padding_idx + 1, seq_len + self.padding_idx + 1)\n",
    "        pos = tf.broadcast_to(pos, tf.shape(x))\n",
    "        x = self.word_embeddings(x)  # (batch_size, seq_len, d_model)\n",
    "        x += self.pos_embeddings(pos)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x, training = training)\n",
    "\n",
    "        # x.shape == (batch_size, seq_len, d_model)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        projected = self.attention_layer(x)  # (batch_size, seq_len, d_model)\n",
    "        logits = tf.squeeze(self.attention_v(projected), 2)  # (batch_size, seq_len)\n",
    "        logits += (tf.squeeze(mask) * -1e9)  # Mask out the padding positions\n",
    "        scores = tf.expand_dims(tf.nn.softmax(logits), 1)  # (batch_size, 1, seq_len)\n",
    "\n",
    "        # x.shape == (batch_size, d_model)\n",
    "        x = tf.squeeze(tf.matmul(scores, x), 1)\n",
    "\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x  # (batch_size, num_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjnLowEgip_u"
   },
   "outputs": [],
   "source": [
    "# Masking\n",
    "def create_padding_mask(seq):\n",
    "    # To be consistent with RoBERTa, the padding index is set to 1.\n",
    "    seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n",
    "\n",
    "    # Add extra dimensions so that we can add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_masks(inp):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    return enc_padding_mask\n",
    "\n",
    "def build_model(model, max_length, vocab_size):\n",
    "    inp = np.ones((1, max_length), dtype = np.int32)\n",
    "    inp[0,:max_length//2] = np.random.randint(2, vocab_size, size = max_length//2)\n",
    "    inp = tf.constant(inp)\n",
    "    enc_padding_mask = create_masks(inp)\n",
    "    _ = model(inp, True, enc_padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opNb8cZUisvC"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, peak_lr, total_steps, warmup_steps):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.peak_lr = peak_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step / self.warmup_steps\n",
    "        arg2 = (self.total_steps - step) / (self.total_steps - self.warmup_steps)\n",
    "        return self.peak_lr * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "CAvn21Cvix75",
    "outputId": "aa61f5c1-4d9b-4bce-ab36-cf267ce18389"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898823/898823 [00:00<00:00, 2804833.10B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 1710255.27B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint at epoch 5 restored!!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from pytorch_transformers import RobertaTokenizer\n",
    "\n",
    "num_layers = 12\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "dff = d_model * 4\n",
    "hidden_act = 'gelu'  # Use 'gelu' or 'relu'\n",
    "dropout_rate = 0.1\n",
    "layer_norm_eps = 1e-5\n",
    "max_position_embed = 514\n",
    "num_emotions = 41  # Number of categories\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "max_length = 100  # Maximum number of tokens\n",
    "buffer_size = 100000\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "peak_lr = 2e-5\n",
    "total_steps = 7000\n",
    "warmup_steps = 700\n",
    "adam_beta_1 = 0.9\n",
    "adam_beta_2 = 0.98\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/checkpoints' # Have to specify path to the checkpoint folder in your Google drive\n",
    "\n",
    "SOS_ID = tokenizer.encode('<s>')[0]\n",
    "EOS_ID = tokenizer.encode('</s>')[0]\n",
    "\n",
    "emobert = EmoBERT(num_layers, d_model, num_heads, dff, hidden_act, dropout_rate,\n",
    "            layer_norm_eps, max_position_embed, vocab_size, num_emotions)\n",
    "\n",
    "build_model(emobert, max_length, vocab_size)\n",
    "\n",
    "learning_rate = CustomSchedule(peak_lr, total_steps, warmup_steps)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = adam_beta_1, beta_2 = adam_beta_2,\n",
    "            epsilon = adam_epsilon)\n",
    "\n",
    "# Define the checkpoint manager.\n",
    "ckpt = tf.train.Checkpoint(model = emobert, optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = None)\n",
    "\n",
    "# Restore the checkpoint at epoch 5 - checkpoint with lowest loss on validation set\n",
    "#print(ckpt_manager.checkpoints[4])\n",
    "ckpt.restore(ckpt_manager.checkpoints[4])\n",
    "print('Checkpoint at epoch 5 restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fxo8UGTvrzBp"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJnVaRYxjhye"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def predict_emotion(uttrs):\n",
    "\n",
    "    bs = 1\n",
    "\n",
    "    uttr_ids = np.ones((len(uttrs), max_length), dtype = np.int32)\n",
    "    i = 0\n",
    "    u = uttrs[0]\n",
    "    u_ids = [SOS_ID] + tokenizer.encode(u)[:(max_length-2)] + [EOS_ID]\n",
    "    uttr_ids[i, :len(u_ids)] = u_ids\n",
    "\n",
    "    uttr_emots = np.zeros((len(uttrs), num_emotions))\n",
    "    num_batches = len(uttrs) // bs\n",
    "    i = 0\n",
    "    s = i * bs\n",
    "    t = s + bs\n",
    "    inp = tf.constant(uttr_ids[s:t])\n",
    "    enc_padding_mask = create_masks(inp)\n",
    "    pred = emobert(inp, False, enc_padding_mask)\n",
    "    pred = tf.nn.softmax(pred).numpy()\n",
    "\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "LXcN2oh5tMNZ",
    "outputId": "e6068b0a-a7fe-4a60-bfbb-80d1858bc909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/a3/1218a3b5651dbcba1699101c84e5c84c36cbba360d9dbf29f2ff18482982/vaderSentiment-3.3.1-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 4.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRawWEM7Kq7S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import plot\n",
    "\n",
    "from sklearn import model_selection, linear_model, preprocessing\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from scipy.stats import norm,skewnorm,pearsonr\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzKd-yn7KMs6"
   },
   "outputs": [],
   "source": [
    "# Download The clean Data set\n",
    "Solution =  pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/BERT/Solution9.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEHw5i3WK3aF"
   },
   "outputs": [],
   "source": [
    "# Array type of the labels\n",
    "labels = np.array(['afraid',#Fear\n",
    "            'angry',#anger\n",
    "            'annoyed',#anger\n",
    "            'anticipating',#Anticipation\n",
    "            'anxious',#Fear\n",
    "            'apprehensive',#Fear\n",
    "            'ashamed',#Disapproval\n",
    "            'caring',#love\n",
    "            'confident',#Trust\n",
    "            'content',#Joy\n",
    "            'devastated',#Sadness\n",
    "            'disappointed',#Sadness\n",
    "            'disgusted',#Disgust\n",
    "            'embarrassed',#Disapproval\n",
    "            'excited',#Joy\n",
    "            'faithful',#Submission\n",
    "            'furious',#anger\n",
    "            'grateful',#Trust\n",
    "            'guilty',#Remorse\n",
    "            'hopeful',#Anticipation\n",
    "            'impressed',#Suprise\n",
    "            'jealous',#Agressiveness\n",
    "            'joyful',#Joy\n",
    "            'lonely',#Sadness\n",
    "            'nostalgic',#Sadness\n",
    "            'prepared',#Anticipation\n",
    "            'proud',#Optimism\n",
    "            'sad',#Sadness\n",
    "            'sentimental',#Sadness\n",
    "            'surprised',#Suprise\n",
    "            'terrified',#Fear\n",
    "            'trusting',#Trust\n",
    "            'agreeing', # neutral\n",
    "            'acknowledging', # neutral \n",
    "            'encouraging', # neutral \n",
    "            'consoling', # neutral \n",
    "            'sympathizing', # neutral \n",
    "            'suggesting', # neutral \n",
    "            'questioning', # neutral \n",
    "            'wishing', # neutral\n",
    "            'neutral']) # neutral \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwtOOos6LDYN"
   },
   "outputs": [],
   "source": [
    "# MAPPING in array type \n",
    "plutchik = np.array(['Fear',\n",
    "            'anger',\n",
    "            'anger',\n",
    "            'Anticipation',\n",
    "            'Fear',\n",
    "            'Fear',\n",
    "            'Disapproval',\n",
    "            'love',\n",
    "            'Trust',\n",
    "            'Joy',\n",
    "            'Sadness',\n",
    "            'Sadness',\n",
    "            'Disgust',\n",
    "            'Disapproval',\n",
    "            'Joy',\n",
    "            'Submission',\n",
    "            'anger',\n",
    "            'Trust',\n",
    "            'Remorse',\n",
    "            'Anticipation',\n",
    "            'Suprise',\n",
    "            'Agressiveness',\n",
    "            'Joy',\n",
    "            'Sadness',\n",
    "            'Sadness',\n",
    "            'Anticipation',\n",
    "            'Optimism',\n",
    "            'Sadness',\n",
    "            'Sadness',\n",
    "            'Suprise',\n",
    "            'Fear',\n",
    "            'trust',\n",
    "            'neutral',\n",
    "            'neutral', \n",
    "            'neutral', \n",
    "            'neutral', \n",
    "            'neutral', \n",
    "            'neutral', \n",
    "            'neutral', \n",
    "            'neutral',\n",
    "            'neutral']) # neutral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06zkZKGJK-TA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "8pQzThv8SfHQ",
    "outputId": "89aa1c8a-16bc-4d73-da65-6df3de7d5ba2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_dialog</th>\n",
       "      <th>character</th>\n",
       "      <th>Text</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>Trent , report .  Have you taken out the tar...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489</td>\n",
       "      <td>Trent</td>\n",
       "      <td>Negative .  He 's got your burn book , and h...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>You don 't have to tell me what would happen...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489</td>\n",
       "      <td>Alphonse</td>\n",
       "      <td>I can 't tell them apart .  They look exactl...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>Okay , then , shoot them both .</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279785</th>\n",
       "      <td>8819128</td>\n",
       "      <td>NOEL</td>\n",
       "      <td>Roz ... take my hand .  Fine , Noel .  I 'll...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279786</th>\n",
       "      <td>8819128</td>\n",
       "      <td>GIL</td>\n",
       "      <td>Roz !  Oh , my God !  Put on a towel , you p...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279787</th>\n",
       "      <td>8819128</td>\n",
       "      <td>KENNY</td>\n",
       "      <td>Oh , I 'm about to faint .  I 'm going to gr...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279788</th>\n",
       "      <td>8819129</td>\n",
       "      <td>ROZ</td>\n",
       "      <td>It 's weird .  My skin tastes kinda salty .</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279789</th>\n",
       "      <td>8819129</td>\n",
       "      <td>BULLDOG</td>\n",
       "      <td>Oh , I 'd say mostly sweet , but a little sa...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195427 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Num_dialog  ... size\n",
       "0              489  ...   11\n",
       "1              489  ...   22\n",
       "2              489  ...   49\n",
       "3              489  ...   12\n",
       "4              489  ...    8\n",
       "...            ...  ...  ...\n",
       "279785     8819128  ...   21\n",
       "279786     8819128  ...   57\n",
       "279787     8819128  ...   48\n",
       "279788     8819129  ...   10\n",
       "279789     8819129  ...   20\n",
       "\n",
       "[195427 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the data set\n",
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_ujx0W0mYZk"
   },
   "outputs": [],
   "source": [
    "# To build a Subset of 100 dialogs (OPTIONAL)\n",
    "liste_num = Solution[\"Num_dialog\"].unique()[0:100]\n",
    "Subset = Solution[Solution[\"Num_dialog\"].isin(liste_num)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0uZ3g7-KMug"
   },
   "outputs": [],
   "source": [
    "# Vader Analyse\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "Solution[\"Vader\"] = Solution.Text.apply(lambda x: analyzer.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGM0bTXkKRy3"
   },
   "outputs": [],
   "source": [
    "# Apply Emo_Bert on every Row.\n",
    "\n",
    "Solution[\"Emo_bert\"] = Solution.Text.apply(lambda x: predict_emotion([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cflbeedPKbvX"
   },
   "outputs": [],
   "source": [
    "# Sort the Emobert distribution\n",
    "Solution[\"Emo_bert_distribution\"]=Solution[\"Emo_bert\"].apply(lambda x: np.sort(x)[-41:][::-1])\n",
    "\n",
    "# Get the id Of Emobert\n",
    "Solution[\"Emo_bert_ids\"] = Solution[\"Emo_bert\"].apply(lambda x: x.argsort()[-41:][::-1])\n",
    "\n",
    "Solution.drop(['Emo_bert'], axis=1, inplace = True)\n",
    "\n",
    "# Get top 3 Emo_bert\n",
    "Solution[\"Emo_Bert\"] = Solution[\"Emo_bert_ids\"].apply(lambda x : labels[x[0:3]] )\n",
    "\n",
    "# Get top 3 plutchik\n",
    "Solution[\"plutchik_basic\"] = Solution[\"Emo_bert_ids\"].apply(lambda x : plutchik[x[0:3]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "leZj7vOcK0M-",
    "outputId": "f002ec95-1c23-40cb-9433-4a6650aad2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is :  599  Dialogs int hte cleanest dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num_dialog</th>\n",
       "      <th>character</th>\n",
       "      <th>Text</th>\n",
       "      <th>size</th>\n",
       "      <th>Vader</th>\n",
       "      <th>Emo_bert_distribution</th>\n",
       "      <th>Emo_bert_ids</th>\n",
       "      <th>Emo_Bert</th>\n",
       "      <th>plutchik_basic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>Trent , report .  Have you taken out the tar...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.9971392, 0.00047829366, 0.00021201778, 0.00...</td>\n",
       "      <td>[38, 37, 5, 25, 32, 0, 19, 28, 39, 8, 14, 24, ...</td>\n",
       "      <td>[questioning, suggesting, apprehensive]</td>\n",
       "      <td>[neutral, neutral, Fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>489</td>\n",
       "      <td>Trent</td>\n",
       "      <td>Negative .  He 's got your burn book , and h...</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.7783</td>\n",
       "      <td>[0.17218097, 0.16828224, 0.06885416, 0.0656215...</td>\n",
       "      <td>[32, 1, 16, 33, 0, 5, 2, 10, 21, 3, 27, 38, 18...</td>\n",
       "      <td>[agreeing, angry, furious]</td>\n",
       "      <td>[neutral, anger, anger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>You don 't have to tell me what would happen...</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>[0.8479019, 0.055057667, 0.013935825, 0.012555...</td>\n",
       "      <td>[38, 37, 5, 19, 35, 32, 0, 34, 33, 21, 14, 1, ...</td>\n",
       "      <td>[questioning, suggesting, apprehensive]</td>\n",
       "      <td>[neutral, neutral, Fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489</td>\n",
       "      <td>Alphonse</td>\n",
       "      <td>I can 't tell them apart .  They look exactl...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.8424796, 0.02681017, 0.02561466, 0.01869111...</td>\n",
       "      <td>[32, 2, 8, 0, 29, 21, 33, 5, 31, 4, 12, 3, 19,...</td>\n",
       "      <td>[agreeing, annoyed, confident]</td>\n",
       "      <td>[neutral, anger, Trust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>Okay , then , shoot them both .</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>[0.24304041, 0.23562402, 0.15192373, 0.0590209...</td>\n",
       "      <td>[37, 35, 32, 40, 34, 33, 38, 1, 31, 30, 0, 10,...</td>\n",
       "      <td>[suggesting, consoling, agreeing]</td>\n",
       "      <td>[neutral, neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>489</td>\n",
       "      <td>Trent</td>\n",
       "      <td>Wait , what ?</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.9697731, 0.018852782, 0.0019101008, 0.00118...</td>\n",
       "      <td>[38, 37, 5, 14, 32, 35, 19, 3, 0, 4, 1, 33, 29...</td>\n",
       "      <td>[questioning, suggesting, apprehensive]</td>\n",
       "      <td>[neutral, neutral, Fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>489</td>\n",
       "      <td>Alphonse</td>\n",
       "      <td>What ?  !</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.9773402, 0.005786574, 0.0050276015, 0.00205...</td>\n",
       "      <td>[38, 37, 5, 0, 14, 4, 32, 1, 25, 27, 28, 23, 3...</td>\n",
       "      <td>[questioning, suggesting, apprehensive]</td>\n",
       "      <td>[neutral, neutral, Fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>489</td>\n",
       "      <td>Kove</td>\n",
       "      <td>Look , Sam and I can 't sit around like Ross...</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.8360</td>\n",
       "      <td>[0.15415718, 0.15093501, 0.128627, 0.11329802,...</td>\n",
       "      <td>[1, 37, 38, 32, 35, 40, 8, 25, 16, 15, 19, 31,...</td>\n",
       "      <td>[angry, suggesting, questioning]</td>\n",
       "      <td>[anger, neutral, neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>489</td>\n",
       "      <td>Sam</td>\n",
       "      <td>Why don 't we use tranqs ?</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.9483739, 0.032491576, 0.0022247022, 0.00182...</td>\n",
       "      <td>[38, 37, 5, 23, 24, 29, 28, 19, 1, 27, 14, 4, ...</td>\n",
       "      <td>[questioning, suggesting, apprehensive]</td>\n",
       "      <td>[neutral, neutral, Fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>492</td>\n",
       "      <td>Sam</td>\n",
       "      <td>I always said he had a lot of ... hang-ups .</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[0.19633222, 0.11917248, 0.077162735, 0.060186...</td>\n",
       "      <td>[40, 6, 2, 11, 21, 18, 20, 13, 5, 29, 12, 32, ...</td>\n",
       "      <td>[neutral, ashamed, annoyed]</td>\n",
       "      <td>[neutral, Disapproval, anger]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Num_dialog  ...                 plutchik_basic\n",
       "0          489  ...       [neutral, neutral, Fear]\n",
       "1          489  ...        [neutral, anger, anger]\n",
       "2          489  ...       [neutral, neutral, Fear]\n",
       "3          489  ...        [neutral, anger, Trust]\n",
       "4          489  ...    [neutral, neutral, neutral]\n",
       "5          489  ...       [neutral, neutral, Fear]\n",
       "6          489  ...       [neutral, neutral, Fear]\n",
       "7          489  ...      [anger, neutral, neutral]\n",
       "8          489  ...       [neutral, neutral, Fear]\n",
       "10         492  ...  [neutral, Disapproval, anger]\n",
       "\n",
       "[10 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"There is : \",Subset.shape[0],\" Dialogs int hte cleanest dataset\")\n",
    "Subset.head(10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Applying_EmoBERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
